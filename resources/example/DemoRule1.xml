<docs>
    <!--SoupLang是WebCollector 2.x中的一种爬虫脚本，以Jsoup内置的CSS SELECTOR为基础
        程序会将SoupLang的脚本(xml)转换成语义树，所以不用担心配置文件会影响网页抽取的速度。  
        SoupLang除了有Jsoup选择元素、元素属性的功能外，还可以做正则匹配、写数据库等操作-->
               
    <!--一个doc对应一个网页,url表示网页的正则。
        如果有多个doc的url，和网页的url匹配，程序会执行多次(所有对应的doc)。
        所以尽量保证一个网页之能匹配到一个doc。
        
        多个doc的设计，是为了爬虫统一管理配置设定的。很多爬虫要多多种不同类型的页面做抽取，抽取
        的代码很难统一管理。利用SoupLang这种脚本，可以将所有的抽取业务写道一个
        
    -->
    <doc url="http://www.zhihu.com/question/[0-9]+">
        
        
        <!--抽取标题 这里抽取到的标题中有一些不需要的东西，可以利用正则过滤
            group表示正则抽取中的group(group为0对应整个匹配字符串，然后依次
            为各个括号中的内容，默认group="1"-->
        <element selector="title">
            <text name="title"  regex="(.*)- 知乎" group="1"/>
        </element>
        
        <!--抽取正文-->
        <element selector="div[id=zh-question-detail]">
            <!--这里如果将name="content"放到element中，name的输出是一个jsoup的Element元素，
                插入到数据库中时，会以Element元素源码的形式插入-->
            <text name="content"/>
        </element>
        
        <!-- 可以看到一些元素有类似 name="xxx"的属性，SoupLang对每个网页，用一个HashMap来维护全局变量，
             凡是有name="xxx"的元素，都会被加入HashMap，key就是name对应的值,value就是元素的输出-->
        
        <!--插入数据库,temp1是程序中，用JDBCHelper.createMysqlTemplate创建template时指定的名称-->
        <sql sql="insert into tb_zhihu_question (title,content) value(?,?)" params="title,content" template="temp1"/>
    </doc>

    <doc url="http://www.zhihu.com/people/.+">
        
        <element selector="div.zm-profile-header">
            
            <!--可以将SoupLang中一条语句的输出，作为另一个语句的输入，在抽取的时候，对DOM树的子树进行抽取，
                往往比在整个DOM树中抽取要简单-->
            <!--根据css selector抽取获得的是数组，使用index可以指定返回数组中第几个元素，默认index="0"-->
            <element selector="span.name" index="0">
                <text name="user"/>
            </element>
        </element>
        
        <sql sql="insert into tb_zhihu_user (user,url) value(?,?)" params="user,url" template="temp1"/>
    </doc>

</docs>